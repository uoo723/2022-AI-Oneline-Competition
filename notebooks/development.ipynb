{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2296b1d2-bd7b-42ef-8784-c5eb5ac7e619",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%config Completer.use_jedi = False\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0029f84f-0ed7-48d3-8d9d-435c5a4852ce",
   "metadata": {},
   "source": [
    "# Import Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d2e5d8c-0c7d-4428-81a2-93b401c3ff44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import OrderedDict, defaultdict\n",
    "from functools import partial\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION\"] = \"python\"\n",
    "\n",
    "from typing import Any, Dict, List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "from joblib import Parallel, delayed\n",
    "from pyserini.search.lucene import LuceneSearcher\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import ConcatDataset, DataLoader, Subset\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer\n",
    "\n",
    "from src.datasets import Dataset, bert_collate_fn\n",
    "from src.metrics import get_mrr\n",
    "from src.monobert.models import MonoBERT\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d77b6ad-6a74-49c9-8299-f3c15848c721",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2057bd20-aac8-40c0-9a19-78edfe9dbae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train data: 1,862\n",
      "# of test data: 827\n"
     ]
    }
   ],
   "source": [
    "train_data_path = \"./data/train.json\"\n",
    "test_data_path = \"./data/test_data.json\"\n",
    "with open(train_data_path, \"r\") as f1, open(test_data_path, \"r\") as f2:\n",
    "    train_data = json.load(f1)\n",
    "    test_data = json.load(f2)\n",
    "test_question = pd.read_csv(\"./data/test_questions.csv\", encoding=\"utf8\")\n",
    "sample = pd.read_csv(\"./data/sample_submission.csv\", encoding=\"utf8\")\n",
    "print(f\"# of train data: {len(train_data['data']):,}\")\n",
    "print(f\"# of test data: {len(test_data['data']):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ea87309c-2606-464d-b28f-c7e93c96d960",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of queries: 233,121\n",
      "# of documents: 137,335\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(\n",
    "    data: Dict[str, Any],\n",
    "    return_query_to_docs: bool = True,\n",
    ") -> Tuple[Dict[str, str], Dict[str, List[str]], Dict[str, str]]:\n",
    "    queries = OrderedDict()\n",
    "    docs = OrderedDict()\n",
    "    query_to_docs = defaultdict(list)\n",
    "\n",
    "    for d in data[\"data\"]:\n",
    "        for paragraph in d[\"paragraphs\"]:\n",
    "            if return_query_to_docs:\n",
    "                for q in paragraph[\"qas\"]:\n",
    "                    queries[q[\"question_id\"]] = q[\"question\"]\n",
    "                    query_to_docs[q[\"question_id\"]].append(paragraph[\"paragraph_id\"])\n",
    "            docs[paragraph[\"paragraph_id\"]] = paragraph[\"context\"]\n",
    "\n",
    "    ret = (queries, docs)\n",
    "    if return_query_to_docs:\n",
    "        ret += (query_to_docs,)\n",
    "    return ret\n",
    "\n",
    "\n",
    "train_queries, train_docs, train_query_to_docs = preprocess_data(train_data)\n",
    "_, test_docs = preprocess_data(test_data, return_query_to_docs=False)\n",
    "test_queries = dict(zip(test_question[\"question_id\"], test_question[\"question_text\"]))\n",
    "\n",
    "print(f\"# of queries: {len(train_queries):,}\")\n",
    "print(f\"# of documents: {len(train_docs):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "17e898cf-855a-4abf-a124-fbc8f60d7742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max length of query: 176\n",
      "Min length of query: 6\n",
      "Avg. length of query: 41.20\n",
      "Std. length of query: 12.99\n",
      "----------------------------------------\n",
      "Max length of document: 676\n",
      "Min length of document: 353\n",
      "Avg. length of document: 508.54\n",
      "Std. length of document: 72.12\n"
     ]
    }
   ],
   "source": [
    "query_lengths = np.array([len(q) for q in train_queries.values()])\n",
    "print(f\"Max length of query: {np.max(query_lengths):,}\")\n",
    "print(f\"Min length of query: {np.min(query_lengths):,}\")\n",
    "print(f\"Avg. length of query: {np.mean(query_lengths):.2f}\")\n",
    "print(f\"Std. length of query: {np.std(query_lengths):.2f}\")\n",
    "print(\"-\" * 40)\n",
    "doc_lengths = np.array([len(d) for d in train_docs.values()])\n",
    "print(f\"Max length of document: {np.max(doc_lengths):,}\")\n",
    "print(f\"Min length of document: {np.min(doc_lengths):,}\")\n",
    "print(f\"Avg. length of document: {np.mean(doc_lengths):.2f}\")\n",
    "print(f\"Std. length of document: {np.std(doc_lengths):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b03b718-4cdc-4fbb-9472-1b820b781ac9",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef161a8c-84d5-43f4-83f0-2bd6275f1150",
   "metadata": {},
   "source": [
    "# BM25 Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e31a2699-f770-4e32-8547-0f97dacf1158",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_contents(data):\n",
    "    contents = []\n",
    "    for d in data[\"data\"]:\n",
    "        for paragraph in d[\"paragraphs\"]:\n",
    "            contents.append(\n",
    "                {\"id\": paragraph[\"paragraph_id\"], \"contents\": paragraph[\"context\"]}\n",
    "            )\n",
    "    return contents\n",
    "\n",
    "\n",
    "data_list = [\n",
    "    (train_data, \"./data/index/train/doc.json\"),\n",
    "    (test_data, \"./data/index/test/doc.json\"),\n",
    "]\n",
    "\n",
    "for data, index_path in data_list:\n",
    "    contents = get_contents(data)\n",
    "    os.makedirs(os.path.dirname(index_path), exist_ok=True)\n",
    "    with open(index_path, \"w\", encoding=\"utf8\") as f:\n",
    "        json.dump(contents, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "705367c6-4b0f-4702-92ef-8dd8f8fabbd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "  --collection JsonCollection \\\n",
    "  --input data/index/train/ \\\n",
    "  --language ko \\\n",
    "  --index data/train.index \\\n",
    "  --generator DefaultLuceneDocumentGenerator \\\n",
    "  --threads 16 \\\n",
    "  --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57453a31-0648-4b2d-8530-b192ba97e8d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m pyserini.index.lucene \\\n",
    "  --collection JsonCollection \\\n",
    "  --input data/index/test/ \\\n",
    "  --language ko \\\n",
    "  --index data/test.index \\\n",
    "  --generator DefaultLuceneDocumentGenerator \\\n",
    "  --threads 16 \\\n",
    "  --storePositions --storeDocvectors --storeRaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e3afc56c-258d-47cf-85d6-e13cc2e74cb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "searcher = LuceneSearcher(\"./data/test.index\")\n",
    "searcher.set_language(\"ko\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "834b0508-13b4-448f-8030-cb2c84a625ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5c16a40f21940bb99e6fd10e4e3a880",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11434 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "answer = []\n",
    "for q in tqdm(test_question[\"question_text\"]):\n",
    "    answer.append(\",\".join([r.docid for r in searcher.search(q, k=10)]))\n",
    "sample[\"paragraph_id\"] = answer\n",
    "sample.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2f213db-b06f-4be4-8790-976da6dde631",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce83a13-56ba-4000-aa37-9d602595c4ce",
   "metadata": {},
   "source": [
    "# monoBERT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dac8ed8-9dcc-466e-b158-3b2f28ccd4b4",
   "metadata": {},
   "source": [
    "## First Stage Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e1737651-a939-4490-ba1e-f5341e35d5c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_shard(\n",
    "    queries: Dict[str, str],\n",
    "    tsv_root_path: str,\n",
    "    tsv_filename: str,\n",
    "    shard_size: int = 10000,\n",
    ") -> None:\n",
    "    query_ids = list(queries.keys())\n",
    "    query_str = list(queries.values())\n",
    "    query_df = pd.DataFrame(\n",
    "        data={\"q_id\": np.arange(len(query_str)), \"query\": query_str}\n",
    "    )\n",
    "    num_shards = (len(query_df) + shard_size - 1) // shard_size\n",
    "\n",
    "    for n in range(num_shards):\n",
    "        tsv_path = os.path.join(tsv_root_path, f\"{tsv_filename}_{n:02d}.tsv\")\n",
    "        os.makedirs(os.path.dirname(tsv_path), exist_ok=True)\n",
    "        query_df[n * shard_size : (n + 1) * shard_size].to_csv(\n",
    "            tsv_path, index=False, header=None, sep=\"\\t\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "be295391-8629-4fa6-9923-0a25d51936ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_query_id_i2s = dict(zip(range(len(train_queries)), train_queries.keys()))\n",
    "make_shard(train_queries, \"./data/top1000\", \"train_queries\")\n",
    "\n",
    "test_query_id_i2s = dict(zip(range(len(test_queries)), test_queries.keys()))\n",
    "make_shard(test_queries, \"./data/top1000\", \"test_queries\", len(test_queries))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b6072d-1352-45c5-908d-a4c6ff5bf9c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!scripts/gen_top1000.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fa95427a-8485-4672-9d24-76e13fc5ea7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.8 s, sys: 1.35 s, total: 21.1 s\n",
      "Wall time: 21.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tsv_path = \"./data/top1000/train_top1000_00.txt\"\n",
    "df = pd.read_csv(tsv_path, sep=\" \", header=None)\n",
    "df[0] = df[0].map(lambda x: query_id_i2s[x])\n",
    "candidates0 = df.groupby(0)[2].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b4447ce9-40d5-43a0-901b-85afc7a37c5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 19.6 s, sys: 1.35 s, total: 21 s\n",
      "Wall time: 20.9 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tsv_path = \"./data/top1000/train_top1000_01.txt\"\n",
    "df = pd.read_csv(tsv_path, sep=\" \", header=None)\n",
    "df[0] = df[0].map(lambda x: query_id_i2s[x])\n",
    "candidates1 = df.groupby(0)[2].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37c6713f-d71b-414e-bf69-17431097248c",
   "metadata": {},
   "source": [
    "## PLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aa0b599f-5b77-46b7-8e74-2a740a30f1b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at monologg/koelectra-base-v3-discriminator were not used when initializing ElectraModel: ['discriminator_predictions.dense_prediction.weight', 'discriminator_predictions.dense.weight', 'discriminator_predictions.dense.bias', 'discriminator_predictions.dense_prediction.bias']\n",
      "- This IS expected if you are initializing ElectraModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ElectraModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "pretrained_model_name = 'monologg/koelectra-base-v3-discriminator'\n",
    "bert = MonoBERT(\n",
    "    pretrained_model_name=pretrained_model_name,\n",
    "    use_layernorm=True,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "_ = bert.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dda2b98-1918-44ca-832a-7a1749fe5906",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "60feac3a-7a6e-4969-8452-9cb70baa7275",
   "metadata": {},
   "outputs": [],
   "source": [
    "shard_size = 10000\n",
    "candidates = {}\n",
    "candidates.update(candidates0)\n",
    "candidates.update(candidates1)\n",
    "\n",
    "train_query_ids =[]\n",
    "train_query_str = []\n",
    "\n",
    "for shard_idx in [0, 1]:\n",
    "    train_query_ids.extend(query_ids[shard_idx * shard_size : (shard_idx + 1) * shard_size]),\n",
    "    train_query_str.extend(query_str[shard_idx * shard_size : (shard_idx + 1) * shard_size]),\n",
    "    \n",
    "train_query_ids = np.array(train_query_ids)\n",
    "train_query_str = np.array(train_query_str)\n",
    "\n",
    "dataset = Dataset(\n",
    "    train_query_ids,\n",
    "    train_query_str,\n",
    "    train_docs,\n",
    "    train_query_to_docs,\n",
    "    candidates,\n",
    "    num_neg=1,\n",
    "    topk=50,\n",
    "    is_training=True,\n",
    ")\n",
    "\n",
    "train_idx, valid_idx = train_test_split(np.arange(len(dataset)), test_size=200)\n",
    "train_dataset = Subset(dataset, train_idx)\n",
    "valid_dataset = Subset(dataset, valid_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "000fb43b-a396-4905-957a-5c691529cea8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    "    collate_fn=partial(bert_collate_fn, tokenizer=tokenizer, max_length=512),\n",
    "    num_workers=0,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    valid_dataset,\n",
    "    batch_size=1,\n",
    "    collate_fn=partial(bert_collate_fn, tokenizer=tokenizer, max_length=512),\n",
    "    num_workers=8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f3c0b868-8c88-4dcf-b1b5-7c0fb3b9de8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a659c4a41a1841ac8114655b6e4e0674",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:01<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n",
      "[W pthreadpool-cpp.cc:90] Warning: Leaking Caffe2 thread-pool after fork. (function pthreadpool)\n"
     ]
    }
   ],
   "source": [
    "valid_dataset.dataset.is_training = False\n",
    "topk = 10\n",
    "topk_candidates = 50\n",
    "\n",
    "bert.eval()\n",
    "indices_list = []\n",
    "for batch_x, batch_y in tqdm(val_dataloader, total=len(valid_dataset)):\n",
    "    batch_x = {k: v.cuda() for k, v in batch_x.items()}\n",
    "    with torch.no_grad():\n",
    "        outputs = bert(batch_x)\n",
    "        scores, indices = torch.topk(outputs, k=topk)\n",
    "        scores = scores.sigmoid().float().cpu()\n",
    "        indices_list.append(indices.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9d4a37b1-a540-4059-a66d-6ae55f2a7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = {}\n",
    "for query_id, rank_indices in zip(train_query_ids[valid_idx], indices_list):\n",
    "    predicted[query_id] = [\n",
    "        candidates[query_id][:topk_candidates][idx] for idx in rank_indices\n",
    "    ]\n",
    "val_y_true = {k: train_query_to_docs[k] for k in train_query_ids[valid_idx]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f516c924-7b35-4e39-82c4-2b8af12ee9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0345436507936508"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrr = get_mrr(val_y_true, predicted)\n",
    "mrr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c5a1d6-af8c-4504-be54-1682b66becfe",
   "metadata": {},
   "source": [
    "## Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f041bd-1117-4add-b4d5-683ca1e62b39",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
